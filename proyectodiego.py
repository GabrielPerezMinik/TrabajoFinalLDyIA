# -*- coding: utf-8 -*-
"""ProyectoDiego.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13G8kb2iJamks6_LM2ZIXL7BvIVThaNPs

Esta es la entrega intermedia del problema de regresión planteado en el Tema 2, debe entregarse según los plazos marcados por el plan de trabajo del módulo.

Esta entrega intermedia debe mostrar una primera versión del proyecto de regresión end-to-end, es decir, debe incluir:
●	Un análisis exploratorio del dataset, explicando los análisis realizados sobre el mismo.
●	Las acciones ejecutadas sobre el dataset como proceso de limpieza de datos, justificando las técnicas utilizadas y las decisiones tomadas.
●	Las acciones ejecutadas sobre el dataset como proceso de transformación de datos, justificando las técnicas utilizadas y las decisiones tomadas.
●	Las acciones ejecutadas sobre el dataset para crear los conjuntos de entrenamiento, validación y test, justificando las decisiones tomadas.
●	El entrenamiento de, al menos, un modelo de regresión, explicando la implementación realizada y mostrando claramente el resultado obtenido.
●	Fragmentos de código (o capturas de pantalla) que demuestren la realización de cada uno de los pasos principales descritos anteriormente.
"""

import calendar
import numpy as np
import pandas as pd
import re
import seaborn as sns

import matplotlib.cm as cm
import matplotlib.pyplot as plt

from scipy.signal import find_peaks

from datetime import datetime, timedelta

from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

import xgboost as xgb


#El encoding parece necesario. De no ponerlo, da un errorde lectura en el encoding utf-8
df = pd.read_csv('data.csv', encoding='latin-1')

"""Comenzamos con el análisis exploratorio.
Primero hacemos un .head para mostrar las primeras lineas del dataset. Esto sirve para saber qué columnas contiene el dataset y saber su tipo.

"""

# Muestra las primeras filas del dataset.
df.head()

"""Después, .info() indica el número de filas, las columnas que tiene y especifica su tipo de dato y el número de valores no nulos que tiene cada una.
.describe() hace unas estadísticas descriptivas de las columnas numéricas del dataset: Media
Desviación estándar
Mínimo
Máximo
Percentiles 25, 50 o mediana y 75
"""

# Obtén información sobre las columnas del dataset.
df.info()

# Genera estadísticas descriptivas del dataset.
df.describe()

"""Después vamos a comprobar si el dataset tiene filas dulpicadas. De tenerlas, ya que uno de los datos es el número de factura (InvoiceNo), y no pueden existir dos facturas iguales, entendemos que la duplicidad es un error y podremos eliminar las filas duplicadas.
Cuidado: por cómo esta hecho el dataset, pone una fila por cada artículo vendido en una factura, por lo que el InvoiceNo sí puede repetirse, siempre que sea un artículo distinto. Por lo tanto, usar solo .duplicated no funciona, al detectar como duplicadas filas de un mismo pedido pero distinto articulo. Para evitarlo, debemos agrupar primero los pedidos por invoice y descripcion, y solo detectar como duplicados los que tengan ambas columnas iguales.

# Identificar duplicados
duplicados = df.duplicated()
print(f"Número de filas duplicadas: {duplicados.sum()}")
filas_duplicadas = df[duplicados]
filas_duplicadas.head(10)  # Mostrar las primeras filas duplicadas
"""

grupos = df.groupby(['InvoiceNo', 'Description'])

conteo_grupos = grupos.size()

duplicados_reales = conteo_grupos[conteo_grupos > 1].reset_index()

filas_duplicadas = pd.merge(duplicados_reales, df, on=['InvoiceNo', 'Description'], how='left')
filas_duplicadas.head(10)

"""Este análisis nos descubre que hay columnas con la misma factura y la misma descripción, pero con cantidades diferentes. En este punto, consideramos que un error así es bastante extraño, y que en realidad esas filas aparentemente duplicadas es más probable que reflejen ventas parciales (que el cliente, en lugar de poner el número que quería en la cesta de la compra añadió varias veces el mismo producto de forma independiente)  o a plazos, por lo que no habría que eliminarlas, si no dejarlas.
Antes, vamos a comprobar si hay alguna fila que sí coincidan esos parámetros pero no el país o el cliente, que sí sería un error.
"""

unicidad_columnas = grupos[['UnitPrice', 'CustomerID', 'Country', 'InvoiceDate']].nunique()

grupos_con_errores = unicidad_columnas[(unicidad_columnas > 1).any(axis=1)]
num_filas_con_errores = filas_duplicadas.shape[0]
print(f"Número de filas con errores: {num_filas_con_errores}")
filas_con_errores = pd.merge(grupos_con_errores.reset_index(), df, on=['InvoiceNo', 'Description'], how='left')
filas_con_errores.head()

"""Encontramos una cosa muy curiosa: La única fila en la que parecen no coincidir es en UnitPrice, lo que podría indicar que hay un error. Pero resulta que ese precio que es distinto es el precio unitario. Y parece haber un patrón en las diferencias (en el head, se ve que uno es el doble que el otro), por lo que sospechamos que estamos viendo promociones por compra en volumen. Vamos a buscar la relación entre los precios e imprimirla, para comprobarlo."""

divisiones_precios = []
for i in range(1, len(filas_con_errores)):
    fila_actual = filas_con_errores.iloc[i]
    fila_anterior = filas_con_errores.iloc[i - 1]
    if fila_actual['Description'] == fila_anterior['Description']:
        # Calculate the division and append it to 'divisiones_precios'
        # Check for zero in the denominator before division
        if fila_anterior['UnitPrice_y'] != 0:
            division = fila_actual['UnitPrice_y'] / fila_anterior['UnitPrice_y']
            divisiones_precios.append(round(division, 1))
        else:
            print(f"Warning: Skipping division for {fila_actual['Description']} due to zero UnitPrice_y in the previous row.")

# Remove infinite or NaN values before plotting
divisiones_precios = [x for x in divisiones_precios if np.isfinite(x)]

print (divisiones_precios)

# Agrupa por 'Description' y obtén una lista de precios únicos
precios_por_descripcion = df.groupby('Description')['UnitPrice'].unique()

# Imprime los precios para cada descripción
for descripcion, precios in precios_por_descripcion.items():
    print(f"Descripción: {descripcion}")
    print(f"Precios: {precios}")
    print("-" * 20)  # Separador visual

"""Muchos de los ratios son, efectivamente de promociones de "segunda unidad al 50%" "tercera al 33% o "cuarta al 25%" o productos gratuitos (UnitPrice 0), aparentemente. Y. en los que no, tenemos productos con descripciones poco específicas que podrían ser productos distintos con la misma descripción (aparece la descripcion Samples, muestras, con muchos precios distintos, por ejemplo, que tiene sentido). Por todo esto, no consideramos que haya filas duplicadas erróneas que afecten a nuestro análisis, y vamos a dejar tal cual el dataframe.

Como hemos visto que las columnas tienen un número distinto de valores no nulos, vamos a hacer una exploración de los valores nulos
"""

valores_faltantes = df.isnull().sum()
print(valores_faltantes)

"""Podemos observar que los valores nulos están en Description, que habría que ver por qué no está puesto qué producto se vendió, y en customerID, que probablemente represente ventas a clientes no habituales o que prefirieron hacer la compra sin identificarse. Este caso no afecta a nuestro propósito, que es predecir las ventas: para eso, no nos importa si el cliente se identifica o no, de modo que los datos que tengas CustomerID nulo nos servirán para nuestro análisis.
Sí interesa más saber por qué faltan descripciones, pero por lo que ha salido en el anterior análisis tenemos la sospecha de que son muestras que se regalan a los clientes o algo parecido, y que no van a tener precio:
"""

# Crear un DataFrame con las filas que tienen valores faltantes en 'Description'
df_missing_description = df[df['Description'].isnull()]

# Explorar las características de este subconjunto de datos
df_missing_description.describe()

# Filtrar las filas con descripción en blanco
filas_descripcion_vacia = df[df['Description'].isnull()]

# Verificar si CustomerID es nulo en todas estas filas
todos_customerid_nulos = filas_descripcion_vacia['CustomerID'].isnull().all()

# Imprimir el resultado
print(f"Todos los CustomerID son nulos en filas con descripción vacía: {todos_customerid_nulos}")

"""Tal como sospechamos, todas las entradas sin descripción no tienen precio, tampoco tienen cliente registrado y además tienen unas cantidades bastante extrañas. Podrían ser, como decimos, algún tipo de muestra que se quiere dejar registro que ha ocurrido o errores de anotación, pero en cualquier caso se pueden eliminar de nuestro análisis, ya que no afectan a las ventas."""

# Eliminar las filas con descripción en nulo
df = df.dropna(subset=['Description'])

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

valores_faltantes = df.isnull().sum()
print(valores_faltantes)

"""Vemos que hay una fila nula en UnitPRice y en Country, vamos a estudiarlas"""

# Detectar filas con UnitPrice nulo
filas_unitprice_nulo = df[df['UnitPrice'].isnull()]
print(filas_unitprice_nulo) #Descomentar para imprimir las filas

# Detectar filas con Country nulo
filas_country_nulo = df[df['Country'].isnull()]
print(filas_country_nulo) #Descomentar para imprimir las filas

"""Vemos que es la misma fila, y que no contiene ninguna información relevante, será una anotación de algo que se rompó, se regaló... Consideramos que podemos eliminarlo"""

# Eliminar las filas con descripción en nulo
df = df.dropna(subset=['UnitPrice', 'Country'])

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

valores_faltantes = df.isnull().sum()
print(valores_faltantes)

"""Confirmamos que solo quedan datos nulos en CustomerID. Al poder haber compradores no identificados, entendemos que son datos válidos, por lo que los dejamos en nuestro análisis.

Ahora vamos a buscar outliers tanto en Quantity como en UnitPrice. Primero vamos a analizar los números negativos, que hemos podido ver que existen, a ver si podemos averiguar qué significan:
"""

# Verificar si hay precios negativos
precios_negativos = df[df['UnitPrice'] < 0]
print("Número de precios negativos:", len(precios_negativos))

# Imprimir las filas con precios negativos (si hay)
if len(precios_negativos) > 0:
    print("\nFilas con precios negativos:")
    print(precios_negativos)

# Contar filas con cantidades negativas
num_filas_negativas = len(df[df['Quantity'] < 0])
print(f"Número de filas con cantidades negativas: {num_filas_negativas}")

# Mostrar filas con cantidades negativas
filas_negativas = df[df['Quantity'] < 0]
print("\nFilas con cantidades negativas:")
print(filas_negativas)

"""Los negativos en precio, según su propia descripción, son correcciones contables, por lo que no tienen cabida en el análisis de venta y los eliminaremos. Los negatiivos en Quantity en cambio parecen devoluciones. Ya que eso sí afecta al beneficio de la empresa, los mantendermos para tenerlos en cuenta."""

df = df[df['UnitPrice'] >= 0]

"""Eliminados los negativos, vamos a ver los outliers."""

# Histograma de UnitPrice
plt.figure(figsize=(8, 6))
sns.histplot(df['UnitPrice'], bins=30)
plt.title('Distribución de UnitPrice (sin valores negativos)')
plt.xlabel('UnitPrice')
plt.ylabel('Frecuencia')
plt.show()

# Box plot de UnitPrice
plt.figure(figsize=(8, 6))
sns.boxplot(y=df['UnitPrice'])
plt.title('Box Plot de UnitPrice (sin valores negativos)')
plt.ylabel('UnitPrice')
plt.show()

"""Observamos que hay algunos valores que sobresalen mucho por arriba, por encima de los 10.000. Como no estamos seguros de qué vende la tienda y podría ser un producto muy exclusivo (que en realidad se podría eliminar por ser poco habitual su tienda, pero queremos ser exahustivos), vamos a analizarlos antes de eliminarlos.

"""

# Obtener los 20 valores más altos de UnitPrice
top_20_unitprice = df['UnitPrice'].nlargest(20)

# Obtener las filas correspondientes a los 10 valores más altos
filas_top_20 = df[df['UnitPrice'].isin(top_20_unitprice)]

# Imprimir las filas
print(filas_top_20)

"""Encontramos tres tipos de datos.
"Adjust bad debt" y "Manual", por su descripción y código nos hace pensar que es una cantidad que se ha puesto a mano por algún motivo contable. En cualquier caso, no parece una venta, así que vamos a eliminarlo. Respecto a "Amazonfee" y "post" son claramente gastos de envío que se cobran al cliente y luego se abonan a la empresa de transporte, así que no es tanto que sean outliers como que son apuntes contables que estorban en nuestro análisis de ventas. De manera que vamos a proceder a eliminar todas las entradas de POST y de AMAZONFEE, al no ser ventas.
"""

# Definir una expresión regular para identificar descripciones de banca y transportes
patron_banca = re.compile(r'(Adjust bad debt|Manual|AMAZON|POST|DOTCOM|Bank|CRUK|SAMPLES)', re.IGNORECASE)

# Eliminar filas que coincidan con el patrón
df = df[~df['Description'].str.contains(patron_banca, na=False)]

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

# Histograma de UnitPrice
plt.figure(figsize=(8, 6))
sns.histplot(df['UnitPrice'], bins=30)
plt.title('Distribución de UnitPrice (sin valores negativos)')
plt.xlabel('UnitPrice')
plt.ylabel('Frecuencia')
plt.show()

# Box plot de UnitPrice
plt.figure(figsize=(8, 6))
sns.boxplot(y=df['UnitPrice'])
plt.title('Box Plot de UnitPrice (sin valores negativos)')
plt.ylabel('UnitPrice')
plt.show()

# Obtener los 10 valores más altos de UnitPrice
top_10_unitprice = df['UnitPrice'].nlargest(20)

# Obtener las filas correspondientes a los 10 valores más altos
filas_top_10 = df[df['UnitPrice'].isin(top_10_unitprice)]

# Imprimir las filas
print(filas_top_10)

"""Los nuevos valores máximos vuelven a ser tarifas de envío, que hay que eliminar igualmente. Luego aparecen comisiones bancarias que, una vez más, son apuntes contables, no ventas.
Respecto a qué hacer con ellos, entendemos que ya que hemos tenido en cuenta los precios descontados por las ventas por volumen antes, también debemos mantener los descuentos, ya que consideramos que es lo mismo pero registrado en el sistema de otra manera. Samples, en cambio, nos parece que es la manera de anotar en el stock las muestras y que no están directamente asociadas a vetnas, así que vamos a eliminarlas también.

Por último vemos un único valor muy alto, que se corresponde a un descuento. Es cierto que habíamos decidido mantenerlos, pero un valor tan anormalmente alto puede corresponder a un error o a un hecho tan puntual que consideramos que no debemos tenerlo en cuenta, ya que puede alterar nuestro análisis posterior
"""

# Obtener el valor máximo de UnitPrice
valor_maximo = df['UnitPrice'].max()

# Eliminar la fila con el valor máximo
df = df[df['UnitPrice'] != valor_maximo]

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

"""Vamos a analizar ahora los outliers de quantity."""

# Histograma de Quantity
plt.figure(figsize=(8, 6))
sns.histplot(df['Quantity'], bins=30)
plt.title('Distribución de Quantity')
plt.xlabel('Quantity')
plt.ylabel('Frecuencia')
plt.show()

# Box plot de Quantity
plt.figure(figsize=(8, 6))
sns.boxplot(y=df['Quantity'])
plt.title('Box Plot de Quantity')
plt.ylabel('Quantity')
plt.show()

"""Observamos algunos valores muy alejados de la media, que además dificultan la visualización. PAra eliminarlos nos apoyamos de la desviación típica"""

# Calcular la media y la desviación estándar de Quantity
media_quantity = df['Quantity'].mean()
desviacion_estandar_quantity = df['Quantity'].std()

# Definir un umbral
umbral_extremos = 20 * desviacion_estandar_quantity

# Filtrar el DataFrame para mostrar filas extremas
filas_extremas = df[(df['Quantity'] < media_quantity - umbral_extremos) | (df['Quantity'] > media_quantity + umbral_extremos)]

# Imprimir las filas extremas
print(filas_extremas)

"""Estos valores extremos no solo no son ventas, si no que se anulan. Hay pues que eliminarlos:"""

# Eliminar filas extremas del DataFrame
df = df[~df.index.isin(filas_extremas.index)]

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

"""El resto de valores muy altos que quedan, si bien están muy lejos de la media, tienen sentido que sean ventas al por mayor por el tipo de producto que son, así que consideramos que ya no son anormales.

Los que sí vamos a valorar tratar también como outliers los valores con precio

*   Elemento de lista
*   Elemento de lista

igual a 0. Algunos pueden ser parte de esas promociones que hemos visto antes, pero otros son también ajustes de inventario para justficar robos o productos dañados, que no son ventas y pueden dar impresión de que se vende más de lo que se vende. Parece ser que los daños, productos perdidos y demás se anotan como ventas de coste 0. Hemos observado que estos datos tienen en común un precio de 0.0, que no tienen cliente y que la cantidad es negativa, por lo que vamos a eliminar las entradas que cumplen esas características.
"""

# Filtrar filas con UnitPrice igual a 0
filas_unitprice_cero = df[df['UnitPrice'] == 0]

# Imprimir el número de filas
print(f"Número de filas con UnitPrice igual a 0: {len(filas_unitprice_cero)}")

# Imprimir las filas
print(filas_unitprice_cero)

# Filtrar filas con UnitPrice igual a 0, CustomerID nulo y Quantity negativa
filas_problematicas = df[(df['UnitPrice'] == 0) & (df['CustomerID'].isnull()) & (df['Quantity'] < 0)]

# Imprimir el número de filas
print(f"Número de filas con UnitPrice igual a 0, CustomerID nulo y Quantity negativa: {len(filas_problematicas)}")

# Imprimir las filas (opcional)
print(filas_problematicas)

# Eliminar filas problemáticas del DataFrame
df = df[~df.index.isin(filas_problematicas.index)]

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

# Filtrar filas con UnitPrice igual a 0
filas_unitprice_cero = df[df['UnitPrice'] == 0]

# Imprimir el número de filas
print(f"Número de filas con UnitPrice igual a 0: {len(filas_unitprice_cero)}")

# Imprimir las filas
print(filas_unitprice_cero)

# Histograma de Quantity
plt.figure(figsize=(8, 6))
sns.histplot(df['Quantity'], bins=30)
plt.title('Distribución de Quantity')
plt.xlabel('Quantity')
plt.ylabel('Frecuencia')
plt.show()

# Box plot de Quantity
plt.figure(figsize=(8, 6))
sns.boxplot(y=df['Quantity'])
plt.title('Box Plot de Quantity')
plt.ylabel('Quantity')
plt.show()

# Obtener los 10 valores más negativos de Quantity
top_10_negativos = df['Quantity'].nsmallest(10)

# Obtener las filas correspondientes a los 10 valores más negativos
filas_top_10_negativos = df[df['Quantity'].isin(top_10_negativos)]

# Imprimir las filas
print("10 valores más negativos de Quantity (filas completas):")
print(filas_top_10_negativos)

# Obtener los 10 valores más positivos de Quantity
top_10_positivos = df['Quantity'].nlargest(10)

# Obtener las filas correspondientes a los 10 valores más positivos
filas_top_10_positivos = df[df['Quantity'].isin(top_10_positivos)]

# Imprimir las filas
print("\n10 valores más positivos de Quantity (filas completas):")
print(filas_top_10_positivos)

"""Con esta limpieza vemos que nos quedan aún filas que no deberían, y probablemente haya más en medio del dataframe. Pero hemos visto una cosa: todas las desscripciones puestas a mano para corregir errores tienen en común que están escritas en minúsuclas, mientras que las del sistema son en mayúsculas. El único problema serían el "cm" de centímetro, el "g" de gramo y el "No" de número, pero podemos excluirlos."""

# Filtrar filas con descripciones que contienen al menos una minúscula, excluyendo "cm", "No" y "g ", y con precio 0.00
filas_con_minusculas = df[df['Description'].str.contains(r'[a-z]', na=False) & (df['UnitPrice'] == 0.00) & ~df['Description'].str.contains(r'(cm|No|\d+g\s)', na=False)]


# Imprimir las filas
print(filas_con_minusculas)

# Eliminar las filas que cumplen con las condiciones
df = df[~df.index.isin(filas_con_minusculas.index)]

# Verificar el número de filas después de la eliminación
print(f"Número de filas después de la eliminación: {df.shape[0]}")

"""Con esto parece que hemos eliminado todas las filas que no deberían estar. Probablemente, con este último análisis más el de la expresión regular del banco habría sido suficiente, hay que hacer más pruebas, se podría haber optimizado.

Vamos a comenzar el análisis. Como queremos predecir las ventas diarias, creo que necesitamos crear nuevas variables. Necesitaremos "Ventas diarias", que sume las quantity agrupadas por día, siempre que esas cantidades sean positivas. "Devoluciones diarias", cuando sean negativas, y "transacciones por día", el total". También necesitaremos "Ingreso diario", multiplicando las cantidades por los precios unitarios, "abonos diarios", multiplicando precios por cantidad negativas, y "beneficio diario", el total. Vamos a crear también dia_semana, para poder estudiar el efecto del día de la semana en las ventas. Lo hacemos con .dayofweek para que los resultados ya estén codificados de 0 a 6, no con etiquetas de texto.
"""

# Crear la columna 'TotalPrice'
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Convertir la columna 'InvoiceDate' a datetime ANTES del groupby
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Crear las variables diarias
df_diario = df.groupby('InvoiceDate').agg(
    Ventas_diarias=('Quantity', lambda x: x[x > 0].sum()),
    Devoluciones_diarias=('Quantity', lambda x: abs(x[x < 0].sum())),  # Aplicar abs() para obtener el valor absoluto
    Transacciones_por_dia=('InvoiceNo', 'nunique'),
    Ingreso_diario=('TotalPrice', lambda x: x[x > 0].sum()),
    Abonos_diarios=('TotalPrice', lambda x: x[x < 0].sum()),
    Beneficio_diario=('TotalPrice', 'sum'),
    dia_semana=('InvoiceDate', lambda x: x.iloc[0].dayofweek)

)


# Ajustar el total de transacciones
df_diario['Transacciones_por_dia'] = df_diario['Ventas_diarias'] + df_diario['Devoluciones_diarias']

# Imprimir el DataFrame resultante
print(df_diario)

"""Antes de aplicar procedimientos de normalización vamos a dividir los datos en entrenamiento y test, ya que si normalizamos los datos antes de separarlos, estaríamos utilizando información del conjunto de prueba para ajustar el escalador (e.g., MinMaxScaler, RobustScaler). Esto se conoce como "fuga de datos" y puede llevar a una sobreestimación del rendimiento del modelo en la evaluación

Necesitamos tomar como datos de entrenamiento el 80% de las ventas hasta el 8 de noviembre de 2011, como validación el 20% restante, y como test las ventas posteriores al 8 de noviembre.
"""

# Convertir la columna 'InvoiceDate' a datetime
df_diario.index = pd.to_datetime(df_diario.index)

# Filtrar los datos entre el 1 de diciembre de 2010 y el 8 de noviembre de 2011
data_train_val = df_diario[(df_diario.index >= datetime(2010, 12, 1)) & (df_diario.index <= datetime(2011, 11, 8))]

# Dividir los datos en conjuntos de entrenamiento y validación (80% y 20%)
df_diario_train, df_diario_val = train_test_split(data_train_val, test_size=0.2, random_state=42)  # random_state para reproducibilidad

# Filtrar los datos para el conjunto de prueba (posteriores al 8 de noviembre de 2011)
df_diario_test = df_diario[(df_diario.index >= datetime(2011, 11, 9)) & (df_diario.index <= datetime(2011, 12, 9))]

# Imprimir el tamaño de los conjuntos
print(f"Tamaño del conjunto de entrenamiento: {df_diario_train.shape[0]}")
print(f"Tamaño del conjunto de validación: {df_diario_val.shape[0]}")
print(f"Tamaño del conjunto de prueba: {df_diario_test.shape[0]}")

"""Una vez divididos, normalizamos con MinMaxScaler, RobustSclaer y StandardScaler, para poder comparar los resultados."""

# Columnas a normalizar
cols_to_scale = ['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'Beneficio_diario', 'dia_semana']

# Crear instancias de los escaladores
minmax_scaler = MinMaxScaler()
robust_scaler = RobustScaler()
standard_scaler = StandardScaler()

# Ajustar los escaladores a los datos de entrenamiento
minmax_scaler.fit(df_diario_train[cols_to_scale])
robust_scaler.fit(df_diario_train[cols_to_scale])
standard_scaler.fit(df_diario_train[cols_to_scale])

# Transformar los datos de entrenamiento, validación y prueba con MinMaxScaler
df_diario_train_minmax = df_diario_train.copy()
df_diario_val_minmax = df_diario_val.copy()
df_diario_test_minmax = df_diario_test.copy()
df_diario_train_minmax[cols_to_scale] = minmax_scaler.transform(df_diario_train_minmax[cols_to_scale])
df_diario_val_minmax[cols_to_scale] = minmax_scaler.transform(df_diario_val_minmax[cols_to_scale])
df_diario_test_minmax[cols_to_scale] = minmax_scaler.transform(df_diario_test_minmax[cols_to_scale])

# Transformar los datos de entrenamiento, validación y prueba con RobustScaler
df_diario_train_robust = df_diario_train.copy()
df_diario_val_robust = df_diario_val.copy()
df_diario_test_robust = df_diario_test.copy()
df_diario_train_robust[cols_to_scale] = robust_scaler.transform(df_diario_train_robust[cols_to_scale])
df_diario_val_robust[cols_to_scale] = robust_scaler.transform(df_diario_val_robust[cols_to_scale])
df_diario_test_robust[cols_to_scale] = robust_scaler.transform(df_diario_test_robust[cols_to_scale])

# Transformar los datos de entrenamiento, validación y prueba con StandardScaler
df_diario_train_standard = df_diario_train.copy()
df_diario_val_standard = df_diario_val.copy()
df_diario_test_standard = df_diario_test.copy()
df_diario_train_standard[cols_to_scale] = standard_scaler.transform(df_diario_train_standard[cols_to_scale])
df_diario_val_standard[cols_to_scale] = standard_scaler.transform(df_diario_val_standard[cols_to_scale])
df_diario_test_standard[cols_to_scale] = standard_scaler.transform(df_diario_test_standard[cols_to_scale])

# Guardar el DataFrame limpio en un archivo CSV
df.to_csv('data_clean.csv', index=False)  # index=False para evitar guardar el índice

"""Ya podemos empezar a entrenar y probar distintos modelos. Nos proponene ARIMA, Prophet, Random Forest, Regresión polinómica, XGBoost y LSTM."""

# Modelo Random Forest
# Función para evaluar el modelo
def evaluar_modelo(modelo, X_train, y_train, X_val, y_val):
    modelo.fit(X_train, y_train)
    predicciones = modelo.predict(X_val)
    rmse = mean_squared_error(y_val, predicciones)
    return rmse

# Define la cuadrícula de hiperparámetros
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
}

# Crea una lista para almacenar los resultados
resultados = []

# Itera sobre los diferentes escaladores
for escalador, datos_train, datos_val, datos_test in [
    (MinMaxScaler(), df_diario_train_minmax, df_diario_val_minmax, df_diario_test_minmax),
    (RobustScaler(), df_diario_train_robust, df_diario_val_robust, df_diario_test_robust),
    (StandardScaler(), df_diario_train_standard, df_diario_val_standard, df_diario_test_standard),
]:
    # Crea una instancia del modelo
    model = RandomForestRegressor(random_state=42)

    # Ajusta el GridSearchCV a los datos de entrenamiento y validación
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring='neg_root_mean_squared_error',
        cv=5,
        n_jobs=-1,
    )
    grid_search.fit(
        pd.concat([datos_train, datos_val])[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']],
        pd.concat([datos_train, datos_val])['Beneficio_diario']
    )

    # Obtén el mejor modelo
    mejor_modelo = grid_search.best_estimator_

    # Evalúa el mejor modelo en el conjunto de prueba
    rmse_test = evaluar_modelo(
        mejor_modelo,
        pd.concat([datos_train, datos_val])[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']],
        pd.concat([datos_train, datos_val])['Beneficio_diario'],
        datos_test[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']],
        datos_test['Beneficio_diario']
    )

    # Guarda los resultados
    resultados.append({
        'escalador': escalador.__class__.__name__,
        'rmse_test': rmse_test,
        'mejores_parametros': grid_search.best_params_,
    })

# Imprime los resultados
for resultado in resultados:
    print(f"Resultados con {resultado['escalador']}:")
    print(f"  RMSE en el conjunto de prueba: {resultado['rmse_test']}")
    print(f"  Mejores parámetros: {resultado['mejores_parametros']}")

"""Predicción con el modelo XGBoost:"""

# Define las variables predictoras (X) y la variable objetivo (y)
X_train = df_diario_train[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']]
y_train = df_diario_train['Beneficio_diario']
X_val = df_diario_val[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']]
y_val = df_diario_val['Beneficio_diario']
X_test = df_diario_test[['Ventas_diarias', 'Devoluciones_diarias', 'Transacciones_por_dia', 'Ingreso_diario', 'Abonos_diarios', 'dia_semana']]
y_test = df_diario_test['Beneficio_diario']


# Crea una instancia del modelo XGBoost Regressor
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Entrena el modelo con los datos de entrenamiento
model.fit(X_train, y_train)

# Realiza predicciones en los datos de validación
y_pred_val = model.predict(X_val)

# Calcula el error cuadrático medio (RMSE) en los datos de validación
rmse_val = mean_squared_error(y_val, y_pred_val)
print(f"RMSE en los datos de validación: {rmse_val}")

# Realiza predicciones en los datos de prueba
y_pred_test = model.predict(X_test)

# Calcula el error cuadrático medio (RMSE) en los datos de prueba
rmse_test = mean_squared_error(y_test, y_pred_test)
print(f"RMSE en los datos de prueba: {rmse_test}")

# Gráfica de la previsión y los datos reales
plt.figure(figsize=(12, 6))
plt.plot(df_diario_test.index, y_test, label='Ventas reales', color='blue')
plt.plot(df_diario_test.index, y_pred_test, label='Previsión XGBoost', color='red')
plt.xlabel('Fecha')
plt.ylabel('Ventas')
plt.title('Previsión de Ventas con XGBoost')
plt.legend()
plt.grid(True)
plt.show()

# Visualizar los residuos (la diferencia entre los valores reales y las predicciones)

residuos = y_test - y_pred_test
plt.hist(residuos, bins=30)
plt.title('Histograma de Residuos')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.show()

plt.scatter(y_pred_test, residuos)
plt.title('Gráfico de Residuos vs. Predicciones')
plt.xlabel('Predicciones')
plt.ylabel('Residuos')
plt.axhline(y=0, color='red', linestyle='--')  # Línea horizontal en y=0
plt.show()

