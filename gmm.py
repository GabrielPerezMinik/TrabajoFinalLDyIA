# -*- coding: utf-8 -*-
"""GMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Eh9iA4R4A-oxj8nZytXS5_-FQiYrL6s

Esta entrega intermedia (análoga a la entrega intermedia 1, que en ese caso era sobre el proyecto de regresión) debe mostrar una primera versión del proyecto de clustering end-to-end, es decir, debe incluir:
●	Un análisis exploratorio del dataset, explicando los análisis realizados sobre el mismo.
●	Las acciones ejecutadas sobre el dataset como proceso de limpieza de datos, justificando las técnicas utilizadas y las decisiones tomadas.
●	Las acciones ejecutadas sobre el dataset como proceso de transformación de datos, justificando las técnicas utilizadas y las decisiones tomadas.
●	El entrenamiento de, al menos, un modelo de clustering, explicando la implementación realizada y mostrando claramente el resultado obtenido.
●	Fragmentos de código (o capturas de pantalla) que demuestren la realización de cada uno de los pasos principales descritos anteriormente.

El modelo que vamos a utilizar es un Gaussian Mixture Model. Parece un buen modelo ya que permite formas más complejas de los clusters que k-means, que asumen que son esféricos, y su enfoque probabilístico permite segmentaciones más flexibles.

Primero, necesitamos importar las librerías necesarias para GMM en Python. Estas son sklearn.mixture para el modelo GMM, pandas para la manipulación de datos, y opcionalmente matplotlib.pyplot para la visualización
"""

import pandas as pd
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt  #Para visualización
from sklearn.preprocessing import StandardScaler #Para estandarizar las variables

import matplotlib.pyplot as plt #Para visualizar con PCA
import matplotlib.colors
from sklearn.decomposition import PCA

"""Después cargamos el dataset con pandas. Habría que limpiarlo, pero viene ya con la limpieza de datos hecha en el apartado 1, por lo que no es necesario llevarlo a cabo."""

df = pd.read_csv('clean_data.csv')

df.head()

"""Elegimos las columnas a estudiar.

index: Es un índice y no aporta información relevante para el clustering. Descartamos.

InvoiceNo: Es un identificador único de la factura, probablemente numérico pero categórico. Descartamos.

StockCode: Es un identificador del producto, aparentemente numérico pero categórico. Descartamos.

Description: Es una descripción textual del producto, no numérica. Descartamos.

Quantity: Representa la cantidad de productos comprados, es numérica y continua. Buena candidata.

InvoiceDate: Es la fecha de la factura, se puede convertir a numérica pero requiere un preprocesamiento. Posible candidata, pero necesita transformación.

UnitPrice: Es el precio unitario del producto, es numérica y continua. Buena candidata.

CustomerID: Es un identificador del cliente, aparentemente numérico pero categórico. Descartamos.

Country: Es el país del cliente, es categórica. Descartar.


Por todo esto vamos a optar por estudiar Quantity y UnitPrice

También vamos a escalar las variables con StandarScaler para evitar que las variables de mayor rango dominen el clustering.
"""

# Selecciona las columnas
X = df[['Quantity', 'UnitPrice']]

# Escala las variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplica GMM a los datos escalados
gmm = GaussianMixture(n_components=3, random_state=0)
gmm.fit(X_scaled)

"""A la hora de hacer el modelo, probamos con 3 y 4 clústeres. Separar en 4 clústeres creaba un clúster bastante más pequeño que el resto de ventas individuales, con Quantity = 1 exactamente de media y desviación estándar de 0. Consideramos que no aporta demasiada información y que esos datos son también ventas habituales, que es lo que se observa al disminuir a tres clústeres (esos datos quedan absorbidos por el clúster de ventas habituales)

Ahora con el método precit del modelo GMM asignamos a cada punto de datos del dataset a un cluster, y añadimos ese label al dataframe original.
"""

labels = gmm.predict(X_scaled)

df['cluster'] = labels

"""Los datos ya están en clusters, queda analizar los resultados. Para ello, vamos a calcular estadísticas descriptivas para cada cluster, y visualizar los clusters"""

# Calcula e imprime las estadísticas descriprivasde cada variable para cada cluster
cluster_stats = df.groupby('cluster')[['Quantity', 'UnitPrice']].agg(['mean', 'median', 'std', 'min', 'max'])
print(cluster_stats)

# Calcula el tamaño de cada cluster
cluster_sizes = df['cluster'].value_counts()
print(cluster_sizes)

"""Al estar utilizando solo dos variables no es necesario reducir la dimensionalidad y se podrían visualizar directamente con un gráfico de dispersión directamente. Sin embargo, PCA puede ayudar a identificar las direcciones de mayor varianza en tus datos. Esto puede ser útil para entender la relación entre las dos variables y cómo contribuyen a la formación de los clusters."""

# Aplica PCA para reducir a 2 dimensiones
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Visualiza los clusters

# Como los colores no se ven bien, los modificamos
colors = ['#00008B', '#FF0000', '#008000']  # Azul oscuro, rojo, verde

# Crea un ListedColormap con los colores
cmap = matplotlib.colors.ListedColormap(colors)

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['cluster'], cmap=cmap)
plt.xlabel('Quantity (escalada)')
plt.ylabel('UnitPrice (escalada)')
plt.title('Clusters de GMM')
plt.show()

"""Para ver un poco mejor los clústeres, vamos a hacer otra visualización de la zona más cercana al (0,0), para poder visualizar los clústeres 0 y 1"""

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['cluster'], cmap=cmap)
plt.xlabel('Quantity (escalada)')
plt.ylabel('UnitPrice (escalada)')
plt.xlim([-0.5, 3.5])
plt.ylim([0, 35])
plt.show()

"""Cluster 0 (rojo): Es el cluster más grande (320445 datos), con una cantidad media de algo más de 6  unidades por compra y un precio unitario medio de 3.06. Podríamos interpretarlo como el cluster de "Compras regulares de bajo valor". Son clientes que compran cantidades moderadas de productos a precios relativamente bajos.

Cluster 1 (azul oscuro): Es un cluster mediano (10254 datos) con una cantidad media de 1 unidad por compra, pero con un precio unitario medio más alto (6.82) y sobre todo muy variable (desviación de 67.64). Además, el rango de UnitPrice es muy amplio, con un máximo de 13541.33. Podría tratarse del cluster de "Compras de alto valor o productos especiales", donde los clientes adquieren productos de alto precio o con un valor unitario muy variable.

Cluster 2 (verde): Es el clúster más pequeño (62554 datos) con una cantidad media de 53 unidades por compra y un precio unitario medio de 1.25. Podríamos interpretarlo como el cluster de "Compras al por mayor", donde los clientes compran grandes cantidades de productos a precios bajos.
"""

df = pd.read_csv('clean_data.csv')

df.head()

# Selecciona las columnas
X = df[['Quantity', 'UnitPrice']]

# Escala las variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplica GMM a los datos escalados
gmm = GaussianMixture(n_components=3, random_state=0)
gmm.fit(X_scaled)

labels = gmm.predict(X_scaled)

df['cluster'] = labels

# Calcula e imprime las estadísticas descriprivasde cada variable para cada cluster
cluster_stats = df.groupby('cluster')[['Quantity', 'UnitPrice']].agg(['mean', 'median', 'std', 'min', 'max'])
print(cluster_stats)

# Calcula el tamaño de cada cluster
cluster_sizes = df['cluster'].value_counts()
print(cluster_sizes)