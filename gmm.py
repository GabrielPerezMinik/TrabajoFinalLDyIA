# -*- coding: utf-8 -*-
"""GMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Eh9iA4R4A-oxj8nZytXS5_-FQiYrL6s

The model we will use is a Gaussian Mixture Model. It seems like a good model since it allows for more complex cluster shapes than k-means, which assumes they are spherical, and its probabilistic approach allows for more flexible segmentations.

First, we need to import the necessary libraries for GMM in Python. These are sklearn.mixture for the GMM model, pandas for data manipulation, and matplotlib.pyplot for visualization
"""

import pandas as pd
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt  # For visualization
from sklearn.preprocessing import StandardScaler # For standardizing the variables

import matplotlib.pyplot as plt # For visualization with PCA
import matplotlib.colors
from sklearn.decomposition import PCA

"""Then we load the dataset using pandas. It would need to be cleaned, but it already comes with the data cleaning done in the previous exercise, so it is not necessary to carry it out."""

df = pd.read_csv('clean_data.csv')

df.head()

"""We choose the columns to study.

index: It is an index and does not provide relevant information for clustering. We discard it.

InvoiceNo: It is a unique identifier of the invoice, probably numerical but categorical. We discard it.

StockCode: It is a product identifier, apparently numerical but categorical. We discard it.

Description: It is a textual description of the product, not numerical. We discard it.

Quantity: It represents the quantity of products purchased, it is numerical and continuous. Good candidate.

InvoiceDate: It is the invoice date, it can be converted to numerical but requires preprocessing. Possible candidate, but needs transformation.

UnitPrice: It is the unit price of the product, it is numerical and continuous. Good candidate.

CustomerID: It is a customer identifier, apparently numerical but categorical. We discard it.

Country: It is the customer's country, it is categorical. Discard.

For all this we are going to choose to study Quantity and UnitPrice.

We are also going to scale the variables with StandardScaler to avoid that the variables with a larger range dominate the clustering.
"""

# Select the columns
X = df[['Quantity', 'UnitPrice']]

# Scale the variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply GMM to the scaled data
gmm = GaussianMixture(n_components=3, random_state=0)
gmm.fit(X_scaled)

"""When building the model, we tested with 3 and 4 clusters. Separating into 4 clusters created a significantly smaller cluster than the rest, consisting of individual sales with a mean Quantity of exactly 1 and a standard deviation of 0. We considered that it did not provide much additional information and that these data points also represent regular sales, which is observed when reducing to three clusters (these data points are absorbed by the regular sales cluster)

Now, using the "predict" method of the GMM model, we assign each data point in the dataset to a cluster, and we add that label to the original dataframe.
"""

labels = gmm.predict(X_scaled)

df['cluster'] = labels

"""The data is now clustered; it remains to analyze the results. To do this, we will calculate descriptive statistics for each cluster and visualize the clusters in a graph"""

# Calculate and print the descriptive statistics of each variable for each cluster
cluster_stats = df.groupby('cluster')[['Quantity', 'UnitPrice']].agg(['mean', 'median', 'std', 'min', 'max'])
print(cluster_stats)

# Calculate the size of each cluster
cluster_sizes = df['cluster'].value_counts()
print(cluster_sizes)

"""Since we are using only two variables, it is not necessary to reduce dimensionality, and they could be visualized directly with a scatter plot. However, PCA can help to identify the directions of greatest variance in your data. This can be useful for understanding the relationship between the two variables and how they contribute to the formation of the clusters."""

# Apply PCA to reduce to 2 dimensions
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Visualize the clusters

# As the colors are not well seen, we modify them
colors = ['#00008B', '#FF0000', '#008000']  # Azul oscuro, rojo, verde

# Create a ListedColormap with the colors
cmap = matplotlib.colors.ListedColormap(colors)

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['cluster'], cmap=cmap)
plt.xlabel('Quantity (escalada)')
plt.ylabel('UnitPrice (escalada)')
plt.title('Clusters de GMM')
plt.show()

"""To see the difference between the clusters a little better, we are going to make another visualization of the area closest to (0,0), so we can visualize cluster 1 more separately"""

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['cluster'], cmap=cmap)
plt.xlabel('Quantity (escalada)')
plt.ylabel('UnitPrice (escalada)')
plt.xlim([-0.5, 3.5])
plt.ylim([0, 35])
plt.show()

"""Cluster 0 (red): It is the largest cluster (320445 data points), with a mean quantity of slightly more than 6 units per purchase and a mean unit price of 3.06. We could interpret it as the "Regular low-value purchases" cluster. These are customers who buy moderate quantities of products at relatively low prices.

Cluster 1 (dark blue): It is a medium-sized cluster (10254 data points) with a mean quantity of 1 unit per purchase, but with a higher mean unit price (6.82) and, above all, very variable (standard deviation of 67.64). In addition, the range of UnitPrice is very wide, with a maximum of 13541.33. It could be the "High-value or special product purchases" cluster, where customers purchase high-priced products or products with a highly variable unit value.

Cluster 2 (green): It is the smallest cluster (62554 data points) with a mean quantity of 53 units per purchase and a mean unit price of 1.25. We could interpret it as the "Wholesale purchases" cluster, where customers buy large quantities of products at low prices.
"""